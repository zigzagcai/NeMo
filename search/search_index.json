{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NVIDIA NeMo","text":"<p>Nvidia NeMo Toolkit</p>"},{"location":"blogs/template/","title":"Notes","text":"<ul> <li>Add folder inside <code>blogs</code> directory with year</li> <li>Remove \"placeholder\" if it exists in the directory</li> <li>Copy the contents of this template to the folder.</li> <li>Edit the contents and update <code>mkdocs.yml</code> at the end to link to the blog post.</li> <li>Send PR and merge.</li> <li>ASSETS: All blog images and external content must be hosted somewhere else. Do NOT add things to github for blog contents!</li> </ul>"},{"location":"blogs/template/#blog-post","title":"Blog Post","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras in massa et lacus consectetur maximus. Donec fringilla, justo vitae condimentum feugiat, est sapien interdum purus, vel rutrum neque ex quis ipsum. Etiam in mauris odio. Mauris in mattis massa. Vivamus tempor libero eu ante aliquet tempor. Vestibulum porttitor odio eu ante posuere, sit amet sagittis quam auctor. Nunc sem sem, ultrices eget porta ac, vulputate non nibh. In tempor risus non felis porta, id scelerisque eros interdum.</p>"},{"location":"blogs/2022/2022-08-introduction/","title":"NeMo Blog Posts and Announcements","text":"<p>NVIDIA NeMo is a conversational AI toolkit that supports multiple domains such as Automatic Speech Recognition (ASR), Text to Speech generation (TTS), Speaker Recognition (SR), Diarization (SDR), Natural Language Processing (NLP), Neural Machine translation (NMT) and much more. NVIDIA RIVA has long been the toolkit that enables efficient deployment of NeMo models. In recent months, NeMo Megatron supports training and inference on large language models (upto 1 trillion parameters !).</p> <p>As NeMo becomes capable of more advanced tasks, such as p-tuning / prompt tuning of NeMo Megatron models, domain adaptation of ASR models using Adapter modules, customizable generative TTS models and much more, we introduce this website as a collection of blog posts and announcements for:</p> <ul> <li>Technical deep dives of NeMo's capabilities</li> <li>Presenting state-of-the-art research results</li> <li>Announcing new capabilities and domains of research that our team will work on.</li> </ul> <p>Visit NVIDIA NeMo to get started</p>"},{"location":"blogs/2023/2023-06-07-fast-conformer/","title":"Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition","text":"<p>The Conformer architecture, introduced by Gulati et al. has been a standard architecture used for not only Automatic Speech Recognition, but has also been extended to other tasks such as Spoken Language Understanding, Speech Translation, and used as a backbone for Self Supervised Learning for various downstream tasks. While they are highly accurate models on each of these tasks, and can be extended for use in other tasks, they are also very computationally expensive. This is due to the quadratic complexity of the attention mechanism, which makes it difficult to train and infer on long sequences, which are used as input to these models due to the granular stride of audio pre-processors (commonly Mel Spectrograms or even raw audio signal in certain models with 10 milliseconds stride). Furthermore, the memory requirement of quadratic attention also significantly limits the audio duration during inference.</p> <p>In this paper, we introduce the Fast Conformer architecture, which applies simple changes to the architecture to significantly reduce the computational cost of training and inference, all while mantaining the strong results of the original Conformer model. We further show that by modifying (on the fly) the global attention module to a linearly scalable attention mechanism - the same model can be used to train (or finetune) and then infer on long sequences (up to 1 hour !).</p> <p>Please refer to the paper here: Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition</p>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#fast-conformer-architecture","title":"Fast Conformer: Architecture","text":"<p>We propose 4 changes to the original Conformer architecture to make it more efficient:</p> <p>1) Downsampling module: The original Conformer paper uses a stack of 2-D Convolutions with a large number of output filters to perform the downsampling in order to reduce the resolution of the incoming audio frames from 10 ms to 40 ms, which makes it more tractable for the subsequent attention layers to operate on. However, these convolutions amount to roughly 20 % of the entire time required to perform a single forward pass of the \"Large\" Conformer (with 120 M parameters). Furthermore, due to the quadratic attention cost, we can obtain a 4x reduction in all subsequent attention layers by downsampling the audio to 80 ms frames. So as the first change, we perform 8x downsampling before any of the Conformer layers are applied. This reduces GMACs to roughly 65% of the baseline Conformer.</p> <p>2) Depthwise convolution: Multiple other works have shown that it is not necessary to use full Convolution layers and that we can save both compute and memory simply by using Depthwise Separable Convoltions. Therefore, we replace all Convolution layers in the preprocessor by Depthwise Seperable Convolutions. This reduces GMACs to roughly 37% of the baseline Conformer.</p> <p>3) Channel reduction: In literature, it is common for the hidden dimension of the downsampling module to match the <code>d_model</code> of the Conformer module for easy application to the subsequent stack of Conformer modules. However, this is not necessary, and we can reduce the number of channels in the downsampling module to reduce the number of parameters and GMACs. We reduce the number of channels to 256, which reduces GMACs to roughly 33% of the baseline Conformer.</p> <p>4) Kernel size reduction: Finally, as we have performed 8x downsampling of the incoming audio, it is no longer required to use the rather large kernel size of 31 in the Convolution layers of the Conformer block. We find that we can roughly reduce it to 9, which preserves the same accuracy while executing slightly faster and reducing the memory cost. This finally reduces GMACs to roughly 33% of the baseline Conformer.</p> <p>Below, we tabulate the accuracy vs speed for each component of Fast Conformer modification schema. Models were tested on LibriSpeech test-other incrementally for each modification starting from the original Conformer. Encoder inference speed (samples/sec) was measured with batch size 128 on 20 sec audio samples. The number of parameters (M) is shown for the encoder only. </p> Encoder WER, Test Other % Inference Samples / sec Params (M) GMACS Baseline Conformer 5.19 169 115 143.2   +8X Stride 5.11 303 115 92.5   +Depthwise conv 5.12 344 111 53.2    +256 channels 5.09 397 109 48.8     +Kernel 9 4.99 467 109 48.7"},{"location":"blogs/2023/2023-06-07-fast-conformer/#fast-conformer-linearly-scalable-attention","title":"Fast Conformer: Linearly Scalable Attention","text":"<p>On an NVIDIA A100 GPU with 80 GB of memory, we find that Conformer reaches the memory limit at around 10-minute long single audio clip. This mean that it is not feasible to perform inference without performing streaming inference which may lead to degraded results.</p> <p>Fast Conformer, due to its 8x stride fairs a little better and can perform inference on roughly 18-minute long audio clips. However, this is still not sufficient for many use cases.</p> <p>We therefore extend Longformer to the Conformer architecture. Longformer uses local attention augmented with global tokens.  We use a single global attention token, which attends to all other tokens and has all other tokens attend to it, using a separate set of query, key and value linear projections, while others attend in a fixed-size window surrounding each token (see below). </p> <p>By switching to limited context attention, we extend the maximum duration that the model can process at once on a single A100 GPU by ~4x: from 10 minutes for Conformer to 18 minutes for Fast Conformer. Furthermore, you can use a pre-trained Fast Conformer model and readily convert its attention to Longformer attention without any further training ! While this will not use the global attention token, it will still be able to process 70-minute long audio clips.`</p>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#checkpoints","title":"Checkpoints","text":"<p>We provide checkpoints for multiple languages on HuggingFace and will add more when we support other languages or tasks.</p> <p>Each of these models are \"Punctuation and Capitalization\" enabled - meaning that they can be used to perform ASR and Punctuation and Capitalization (PnC) in a single pass and can produce text that is more natural to read. Post-processing to normalize text will be provided in a future release.</p> <p>Some languages we currently support for ASR are :</p> <ul> <li>English</li> <li>French</li> <li>German</li> <li>Italian</li> <li>Spanish</li> <li>Belarusian</li> <li>Croatian</li> <li>Polish</li> <li>Ukranian</li> <li>Russian</li> </ul>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#data-processing","title":"Data Processing","text":"<p>When constructing datasets with support for Punctuation and Capitalization, dataset preparation is an important piece of the puzzle. When training a model with Punctuation and Capitalization, you may face the following issues:</p> <p>1) During training, there may be the case the standard evaluation benchmarks do not have punctuation or capitalization, but the model still predicts them, providing an incorrect evaluation of model training. </p> <p>2) Not all training data may have punctuation or capitalization, so you may want to filter out such samples to prevent confusing the model about whether it should predict punctuation or capitalization.</p> <p>In order to provide a consistent and reproducible way to process such dataset, we will begin providing the dataset preprocessing strategies in Speech Data Processor.</p> <p>Speech Dataset Processor currently hosts dataset processing recipies for Spanish, and we will add more languages in the future.</p>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#usage","title":"Usage","text":"<p>Fast Conformer can be instantiated and used with just a few lines of code when the NeMo ASR library is installed.</p>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#global-attention","title":"Global Attention","text":"<p>For global attention on modest files (upto 15-18 minutes on an A100), you can perform the following steps :</p> <pre><code>from nemo.collections.asr.models import ASRModel\nmodel = ASRModel.from_pretrained(\"nvidia/stt_en_fastconformer_hybrid_large_pc\")\nmodel.transcribe([\"&lt;path to a audio file&gt;.wav\"])  # ~10-15 minutes long!\n</code></pre>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#local-attention","title":"Local Attention","text":"<p>Coming in NeMo 1.20, you can easily modify the attention type to local attention after building the model. Then you can also apply audio chunking for the subsampling module to perform inference on huge audio files!</p> <p>For local attention on huge files (upto 11 hours on an A100), you can perform the following steps :</p> <pre><code>from nemo.collections.asr.models import ASRModel\nmodel = ASRModel.from_pretrained(\"nvidia/stt_en_fastconformer_hybrid_large_pc\")\nmodel.change_attention_model(\"rel_pos_local_attn\", [128, 128])  # local attn\n# (Coming in NeMo 1.20)\nmodel.change_subsampling_conv_chunking_factor(1)  # 1 = auto select\nmodel.transcribe([\"&lt;path to a huge audio file&gt;.wav\"])  # 10+ hours !\n</code></pre>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#results","title":"Results","text":"<p>By performing the simple modifications in Fast Conformer, we obtain strong scores throughout multiple speech tasks as shown below, all while having more efficient training and inference.</p> <p>For ASR alone, we obtain a 2.8x speedup as compared to Conformer encoder of similar size for inference, and can use larger batch sizes during training to further speedup training. We also compare results against tasks such as Speech Translation and Spoken Language Understanding in order to validate that these changes lead only to improvements in efficiency and do not degrade accuracy on downstream tasks.</p>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#asr-results","title":"ASR Results","text":"<p>Below, we list some of our results on Fast Conformer on LibriSpeech test-other. We compare against the original Conformer and other recent efficient architectures. We compare against the Efficient Conformer from Burchi2021 which uses a progressive downsampling of the Conformer architecture. We also compare against Kim2022 SqueezeFormer which uses a U-Net like architecture to progressively downsample the input and upsample it to 40 ms resolution prior to applying the decoder. </p> <p>We find that Fast Conformer is able to achieve the same accuracy as the Conformer while being 2.8x faster and using fewer parameters. </p> <p>Fast Conformer-Large with CTC and RNNT decoders trained on Librispeech. Greedy WER (%) was measured on Librispeech test-other.  The number of parameters  (M) and compute (Multiply-Acc, GMAC) are shown for encoder only.</p> Encoder WER, % Params, Compute, test-other M GMACS RNNT decoder Conformer 5.19 115 143.2 Fast Conformer 4.99 109 48.7 CTC decoder Conformer 5.74 121 149.2 Eff. Conformer [Burchi2021] 5.79 125 101.3 SqeezeFormer [Kim2022] 6.05 125 91.0 Fast Conformer 6.00 115 51.5"},{"location":"blogs/2023/2023-06-07-fast-conformer/#speech-translation-results","title":"Speech Translation Results","text":"<p>We also evaluate Fast Conformer on the IWSLT 2014 German-English speech translation task. We find that Fast Conformer is able to achieve the same accuracy as the Conformer while being upto 1.8x faster and using fewer parameters.</p> <p>Our models have been trained on all available datasets from IWSLT22 competition which corresponds to 4k hours of speech. Some of the datasets did not contain German translations, so we generated them ourselves with text-to-text machine translation model trained on WMT21 and in-domain finetuned on Must-C v2.</p> <p>Speech Translation, MUST-C V2 tst-COMMON dataset. SacreBLEU, total inference time, and relative inference speed-up were measured with a batch size \\(32\\) for two speech translation models with Conformer-based encoder and either RNNT, or Transformer decoder.</p> Encoder BLEU Time (sec) Speed-up Transformer decoder Conformer 31.0 267 1X Fast Conformer 31.4 161 1.66X RNNT decoder Conformer 23.2 83 1X Fast Conformer 27.9 45 1.84X"},{"location":"blogs/2023/2023-06-07-fast-conformer/#spoken-language-understanding-results","title":"Spoken Language Understanding Results","text":"<p>For the Speech Intent Clasification and Slot Filling task, experiments are conducted using the SLURP dataset, where intent accuracy and SLURP-F1 are used as the evaluation metric. For a fair comparison, both Conformer and Fast Conformer models are initialized by training on the same NeMo ASR Set dataset (roughly 25,000 hours of speech) and then the weights of the entire model are finetuned with the respective decoders.</p> <p>Speech intent classification and slot filling on SLURP dataset. ESPNet-SLU  and SpeechBrain-SLU models use a HuBERT encoder pre-trained via a self-supervised objective on LibriLight-60k.  Inference time and relative speed-up against Conformer are measured with batch size 32.</p> Model Intent Acc SLURP F1 Inference, sec Rel. Speed-up SpeechBrain-SLU 87.70 76.19 - - ESPnet-SLU 86.52 76.91 - - Conformer/Fast Conformer+Transformer Decoder Conformer 90.14 82.27 210 1X Fast Conformer 90.68 82.04 191 1.1X"},{"location":"blogs/2023/2023-06-07-fast-conformer/#long-form-speech-recognition-results","title":"Long Form Speech Recognition Results","text":"<p>While Fast Conformer can be modified post training to do simple inference on long audio, due to the mismatch in attention window with limited future information, the model's WER may degrade a small amount. Users can therefore add global token followed by subsequent fine-tuning for some small steps on the same dataset in order to significantly recover (and outperform) the original models WER.</p> <p>Note that with Longformer style attention, we can still perform buffered inference with large chunk size - upto 1 hour long, therefore inference on even longer audio can be done efficiently with few inference steps.</p> <p>Fast Conformer versus Conformer on long audio. We evaluated four versions of FastConformer: (1) FC with global attention (2) FC with limited context (3) FC with limited context and global token (4) FC with limited context and global token, trained on long concatenated utterances. Models have been evaluated on two long-audio bencmarks: TED-LIUM v3 and Earning 21. Greedy WER(%).</p> Model TED-LIUM v3 Earnings21 Conformer 9.71 24.34 Fast Conformer 9.85 23.84  + Limited Context 9.92 28.42   + Global Token 8.00 20.68    + Concat utterances 7.85 19.52"},{"location":"blogs/2023/2023-08-forced-alignment/","title":"How does forced alignment work?","text":"<p>In this blog post we will explain how you can use an Automatic Speech Recognition (ASR) model1 to match up the text spoken in an audio file with the time when it is spoken. Once you have this information, you can do downstream tasks such as:</p> <ul> <li> <p>creating subtitles such as in the video below2 or in the Hugging Face space</p> </li> <li> <p>obtaining durations of tokens or words to use in Text To Speech or speaker diarization models</p> </li> <li> <p>splitting long audio files (and their transcripts) into shorter ones. This is especially useful when making datasets for training new ASR models, since audio files that are too long will not be able to fit onto a single GPU during training. 3</p> </li> </ul> <p> </p> Video with words highlighted according to timestamps obtained with NFA"},{"location":"blogs/2023/2023-08-forced-alignment/#what-is-forced-alignment","title":"What is forced alignment?","text":"<p>This task of matching up text to when it is spoken is called 'forced alignment'. We use 'best alignment', 'most likely alignment' or sometimes just 'the alignment' to refer to the most likely link between the text and where in the audio it is spoken. Normally these links are between chunks of the audio and the text tokens4. If we are interested in word alignments, we can simply group together the token alignments for each word.</p> <p> </p> Diagram of a possible alignment between audio with 5 timesteps and text with 3 tokens ('C', 'A', 'T') <p>The 'forced' in 'forced alignment' refers to the fact that we provide the reference text ourselves and use the ASR model to get an alignment based on the assumption that this reference text is the real ground truth, i.e. exactly what is spoken - sometimes it makes sense to drop this requirement in case your reference text is incorrect. There are various other aligners that work on this assumption5.</p> <p>Sometimes in discussing this topic, we may drop the 'forced' and just call it 'alignment' when we mean 'forced alignment'. We will sometimes do this in this tutorial, for brevity.</p> <p> </p> We can think of an alignment as a way to arrange the S number of tokens into T number of boxes <p>In forced alignment our two inputs are the audio and the text. You can think of the audio as being split into \\(T\\) equally-sized chunks, or 'timesteps', and the text as being a sequence of \\(S\\) tokens. So we can think of an alignment as either a mapping from the \\(S\\) tokens to the \\(T\\) timesteps, or as a way of duplicating some of the tokens so that we have a sequence of \\(T\\) of them, each being mapped to the timestep when it is spoken. Thus this alignment algorithm will only work if \\(T \\ge S\\).</p> <p>The task of forced alignment is basically figuring out what the exact \\(T\\)-length sequence of these tokens should be in order to give you the best alignment.</p>"},{"location":"blogs/2023/2023-08-forced-alignment/#formulating-the-problem","title":"Formulating the problem","text":"<p>To do forced alignment, we will need an already-trained ASR model 6. This model's input is the spectrogram of an audio file, which is a representation of the frequencies present in the audio signal. The spectrogram will have \\(T_{in}\\) timesteps. The ASR model will output a probability matrix of size \\(V \\times T\\) where \\(V\\) is the number of tokens in our vocabulary (e.g. the number of letters in the alphabet of the language we are transcribing) and \\(T\\) is the number of output timesteps. \\(T\\) may be equal to \\(T_{in}\\), or it may be smaller by some ratio if our ASR model has some downsampling in its neural net architecture. For example, NeMo's pretrained models have the following downsampling ratios:</p> <ul> <li>NeMo QuartzNet models have \\(T = \\frac{T_{in}}{2}\\)</li> <li>NeMo Conformer models have \\(T = \\frac{T_{in}}{4}\\)</li> <li>NeMo Citrinet and FastConformer models have \\(T = \\frac{T_{in}}{8}\\)</li> </ul> <p>In the diagram below, we have drawn \\(T_{in} = 40\\) and \\(T = 5\\), as one would obtain from one of the pretrained NeMo Citrinet or FastConformer models, which have a downsampling ratio of 8. In terms of seconds, as spectrogram frames are 0.01 seconds apart, each column in the spectrogram corresponds to 0.01 seconds, and each column in the ASR Model output matrix corresponds to \\(0.01 \\times 8 = 0.08\\) seconds.</p> <p> </p> The input audio contains T_{in} timesteps. The matrix outputted by the ASR Model has shape V x T <p>As mentioned above, the ASR Model's output matrix is of size \\(V \\times T\\). The number found in row \\(v\\) of column \\(t\\) of this matrix is the probability that the \\(v\\)-th token is being spoken at timestep \\(t\\). Thus, all the numbers in a given column must add up to 1.</p> <p>If we didn't know anything about what is spoken in the audio, we would need to \"decode\" this output matrix to produce the best possible transcription. That is a whole research area of its own - a topic for another day.</p> <p>Our task is forced alignment, where by definition we have some reference text matching the ground truth text that is actually spoken (e.g. \"cat\") and we want to specify exactly when each token is spoken.</p> <p>As mentioned in the previous section, we essentially have \\(T\\) slots, and we want to fill each slot with the tokens <code>'C', 'A', 'T'</code> (in that order) in the locations where the sound of each token is spoken.</p> <p>To make sure we go through the letters in order, we can think of this \\(T\\)-length sequence as being one which passes through the graph below from start to finish, making a total of \\(T\\) stops on the red tokens.</p> <p> </p> Every possible alignment passes from \"START\" to \"END\" with T stops at each red token <p>For now we ignore the possibility of some of the audio not containing any speech and the possibility of a 'blank' token, which is a key feature of CTC (\"Connectionist Temporal Classification\") models \u2014 we will get to that later.</p> <p>Let's look at the (made-up) output of our ASR model. We've removed all the tokens from our vocabulary except <code>CAT</code> and normalized the columns to make the maths easier for our example.  We denote the values in this matrix as \\(p(s,t)\\), where \\(s\\) is the index of the token in the ground truth, and \\(t\\) is the timestep in the audio.</p> Timestep: 1 2 3 4 5 C 0.7 0.4 0.1 0.1 0.1 A 0.1 0.3 0.2 0.4 0.2 T 0.2 0.3 0.7 0.5 0.7 <p>Our first instinct may be to try to take the argmax of each column in \\(p(s,t)\\), however that may lead to an alignment which does not match the order of tokens in the reference text, or may leave out some tokens entirely. In the current example, such a strategy will yield <code>C (0.7) -&gt; C (0.4) -&gt; T (0.7) -&gt; T (0.5) -&gt; T (0.5)</code>, which spells <code>CT</code> instead of <code>CAT</code>.</p>"},{"location":"blogs/2023/2023-08-forced-alignment/#forced-alignment-the-naive-way","title":"Forced alignment the naive way","text":"<p>The issue with the above attempt is we did not restrict our search to only alignments that would spell out <code>CAT</code>.</p> <p>Since our number of timesteps (\\(T=5\\)) and tokens (\\(S=3\\)) is small, we can list out every possible alignment, i.e. every possible arrangement of <code>'CAT'</code> that will fit in our 5 slots:</p> Timestep: 1 2 3 4 5 alignment 1 C C C A T alignment 2 C C A A T alignment 3 C C A T T alignment 4 C A A A T alignment 5 C A A T T alignment 6 C A T T T <p>Each token has a certain probability of being spoken at each time step, determined by our ASR model. The probability of a particular sequence of tokens is calculated by multiplying together the individual probabilities of each token at each timestep. Assuming our ASR model is a good one, the best alignment is the one with the highest cumulative probability.</p> <p>Let's show the \\(p(s,t)\\) probabilities again.</p> Timestep: 1 2 3 4 5 C 0.7 0.4 0.1 0.1 0.1 A 0.1 0.3 0.2 0.4 0.2 T 0.2 0.3 0.7 0.5 0.7 <p>We can calculate the probability of each possible alignment by multiplying together each \\(p(s,t)\\) that it passes through:</p> Timestep: 1 2 3 4 5 Total probability of alignment alignment 1 C C C A T0.7 * 0.4 * 0.1 * 0.4 * 0.7 = 0.008 alignment 2 C C A A T 0.7 * 0.4 * 0.2 * 0.4 * 0.7 = 0.016 alignment 3 C C A T T0.7 * 0.4 * 0.2 * 0.5 * 0.7 = 0.020 alignment 4 C A A A T0.7 * 0.3 * 0.2 * 0.4 * 0.7 = 0.012  alignment 5 C A A T T0.7 * 0.3 * 0.2 * 0.5 * 0.7 = 0.015 alignment 6 C A T T T0.7 * 0.3 * 0.7 * 0.5 * 0.7 = 0.051 &lt;- the max <p>We can see that the most likely path is <code>'CATTT'</code> because it has the highest total probability. In other words, based on our ASR model, the most likely alignment is that <code>'C'</code> was spoken at the first timestep, <code>'A'</code> was spoken at the second timestep, and <code>'T'</code> was spoken for the last 3 timesteps. </p>"},{"location":"blogs/2023/2023-08-forced-alignment/#the-naive-way-but-listing-all-the-possible-paths-using-a-graph","title":"The naive way but listing all the possible paths using a graph","text":"<p>To make further progress in understanding forced alignment, let's list all the possible paths in a systematic way by arranging them in a tree-like graph like the one below. </p> <p>We initialize the graph with a 'start' node (for clarity), then connect it to nodes representing the tokens that our alignment can have at the first timestep (<code>t=1</code>). In our case, this is only a single token <code>C</code>. From that <code>C</code> node, we draw arrows to 2 other nodes. The higher node represents staying at the same token (<code>C</code>) for the next timestep (<code>t=2</code>). The lower node represents going to the next token (<code>A</code>) for the next timestep (<code>t=2</code>). We continue this process until we have drawn all the possible paths through our reference text tokens for the fixed duration \\(T\\). We do not include paths that reach the final token too early or too late.</p> <p>We end up with the tree below, which represents all the possible paths through the <code>CAT</code> tokens over 5 timesteps. </p> <p>You can check for yourself that every alignment we listed in the table in the previous section is represented as a path from left to right in this tree.</p> <p>We can label each node in the graph with its \\(p(s,t)\\) probabilities (dark green).</p> <p>Let's also calculate the cumulative product along each path and include that as well (light green).</p> <p>Once we do that, we can look at all the nodes at the final timestep, and see that the cumulative product at each node is the cumulative probability of the path from start to T that lands at that node. </p> <p>As before, we can see that the probability of the most likely path, i.e. the most likely alignment, is 0.051. If we trace back our steps from that T node to the start, then we see that the path is <code>'CATTT'</code>.7</p> <p> </p> This graph lists every possible alignment. The most likely alignment becomes highlighted in purple."},{"location":"blogs/2023/2023-08-forced-alignment/#the-trouble-with-longer-sequences","title":"The trouble with longer sequences","text":"<p>The naive method in the previous sections was intuitive, easy to calculate and gave us the correct answer, but it was only feasible because we had such a small number of possible alignments. In an utterance of 10 words over 5 seconds, conservatively you can expect 20 tokens and 63 timesteps8 - that would lead to about \\(4.8 \\times 10^{15}\\) possible alignments9! </p> <p>Fortunately there is a method to obtain the most likely alignment, for which you:</p> <ul> <li>don't need to calculate all the cumulative products for every path, and</li> <li>don't even need to draw the full tree graph.</li> </ul> <p>We will work towards this method in the next sections.</p>"},{"location":"blogs/2023/2023-08-forced-alignment/#spotting-graph-redundancies","title":"Spotting graph redundancies","text":"<p>Fortunately, because we are only interested in the highest probability path from the start node to one of the final <code>T</code> nodes on the right, this means that the tree graph has a lot of redundant nodes that we don't need to worry about.</p> <p>For example, consider the two nodes in the tree corresponding to token <code>A</code> (<code>s=2</code>) at time <code>t=3</code>. Looking at the cumulative products of these two nodes (in light green), we can see that the top <code>A</code> node (corresponding to the partial path <code>CCA</code>) has a cumulative product of 0.056, while the bottom <code>A</code> node (corresponding to the partial path <code>CAA</code>) has a smaller cumulative product of 0.042.</p> <p>The paths downstream of the top node are identical in graph structure to those downstream of the bottom node, as are the \\(p(s,t)\\) values (in dark green) of any nodes in these downstream paths. Therefore, since the cumulative product of any path downstream of an <code>A</code> node at <code>t=3</code> will just be the cumulative product of that <code>A</code> node at <code>t=3</code> multiplied by these downstream \\(p(s,t)\\) values, it is impossible for any path downstream of the bottom <code>A</code> node to have a higher cumulative product than the corresponding path downstream of the top <code>A</code> node. Thus, it is impossible that any of the paths downstream of the bottom <code>A</code> node will end up being the optimal path overall, and we can safely discard them. This is shown in the animation below.</p> <p> </p> If we examine the two 'A' nodes at 't=3', we see that we can discard the node with the lower cumulative product, and the nodes downstream of it. <p>These redundancies exist between any nodes with the same \\(s\\) and \\(t\\) values. All of their downstream nodes will have the same structure, but we only need to keep the node that corresponds to the most probable path from the start node to (s,t).</p> <p>So, for each \\((s,t)\\) we only need to record the most probable path to (s,t) and can discard the non-winning node and its downstream nodes. Discarding the downstream nodes means that we will have fewer nodes to look at in the next timesteps, helping to keep the number of computations required relatively low.</p> <p>The animation below shows this. We start with all nodes in their original colors, and color nodes red if they represent the most probable path to (s,t). When there is only one node in the graph that has a particular \\((s,t)\\) value, it by default is the most probable path to (s,t), so we color it red immediately. When there is more than one node with the same \\((s,t)\\) value, we look at the cumulative probability (in light green) of these nodes. We mark the node with the lower cumulative probability dark blue, meaning we calculated its cumulative probability but realized that we can discard it. We mark its downstream nodes dark gray, to indicate that we can discard them, and don't need to consider them in future timesteps. Finally, for the node with the higher cumulative probability, we mark it red, to indicate that it is the most probable path to (s,t).</p> <p> </p> We cycle through all possible (s,t) and discard the nodes that we do not need. <p>The cumulative product of the remaining node at the final timestep is the probability of the most likely alignment path. Again, we can trace back the steps from that node to the start to recover the exact path that gives that probability (<code>'CATTT'</code>).</p> <p>Although we show the cumulative probabilities for all nodes in the animation (for illustrative purposes), we didn't need to calculate the cumulative probabilities for any of the nodes that are dark gray, but we still managed to find the most likely path. We obtained the same result as with naive graph method but a lot fewer operations.</p>"},{"location":"blogs/2023/2023-08-forced-alignment/#formalizing-the-efficient-forced-alignment-algorithm","title":"Formalizing the efficient forced alignment algorithm","text":"<p>Let\u2019s formalize the steps we followed. If we look at the nodes that we didn\u2019t discard, they form a different shape of graph. The animation below transforms the tree graph into its new shape by hiding the discarded nodes behind the non-discarded nodes.</p> <p> </p> We create a different shape of graph using only the nodes that we did not discard. <p>The resulting graph has a single node for each \\((s,t)\\). This graph is often referred to as a trellis. Each node has an associated number in red which is the probability of the most likely path from start to \\((s,t)\\). (We also keep the \\(p(s,t)\\) values in dark green for illustrative purposes).</p> <p>We can recover the most likely alignment by looking at the node at \\((S,T)\\). Its cumulative probability, in red, is the probability of the most likely alignment. We can also recover the exact path that has that probability by tracing following the non-discarded edges (in light gray) backwards to the start token. This produces the <code>'CATTT'</code> sequences yet again.</p> <p>At each node \\((s,t)\\), the procedure we used to calculate the most probable path to \\((s,t)\\), was to look at the tokens in the previous timestep that could have  transitioned into this \\((s,t)\\), calculate the candidates for the most probable path to \\((s,t)\\), and pick the maximum value.</p> <p>In our scenario, where at each timestep the token either stays the same or moves onto the next one, candidates come from \\((s-1, t-1)\\) &amp; \\((s,t-1)\\). </p> <p>We can denote this formula as:</p> <p><code>prob-of-most-likely-path-to(s, t) = max (prob-of-most-likely-path-to(s-1, t-1) * p(s,t), prob-of-most-likely-path-to(s, t-1) * p(s,t))</code>.</p> <p>Let's make the formula shorter by denoting <code>prob-of-most-likely-path-to(s-1, t-1)</code> using the letter 'v' (to be explained later). So the formula becomes: </p> \\[v(s, t) = \\max (v(s-1, t-1) \\times p(s,t), v(s, t-1) \\times p(s,t))\\] <p>We can also show the rule on a subsection of our trellis graph, as below.</p> <p> </p> The formula we use to calculate v(s,t) probabilities in our current setup."},{"location":"blogs/2023/2023-08-forced-alignment/#the-viterbi-algorithm","title":"The Viterbi algorithm","text":"<p>What we described above is actually known as the Viterbi algorithm. What we denoted <code>prob-of-most-likely-path-to(s, t)</code> is often denoted <code>v(s,t)</code>, A.K.A. the Viterbi probability (of token <code>s</code> at time <code>t</code>).</p> <p>The Viterbi algorithm is an efficient method for finding the most likely alignment, and exploits the redundancies in the tree graph discussed above. It involves creating a matrix of size \\(S \\times T\\) (called a 'Viterbi matrix') and populating it column-by-column. </p> <p>The shape of the initialized Viterbi matrix for our scenario is shown below. Every element in the matrix corresponds to a node in the trellis (except for elements in the bottom left and top right of the matrix, which we did not draw in our trellis as they would not form valid alignments).</p> t=1 t=2 t=3 t=4 t=5 s=0 AKA token is C ?? ?? ?? ?? ?? s=1 AKA token is A ?? ?? ?? ?? ?? s=2 AKA token is T ?? ?? ?? ?? ?? <p>We need to fill in every element in the Viterbi matrix column-by-column and row-by-row10. Once we have finished, \\(v(s=S,t=T)\\) will contain the probability of the most likely path to (S,T), i.e. from start to finish, and if we recorded the argmax for computing each \\((s,t)\\), then we can use these values to recover what the exact sequence is which has this highest probability. The recorded argmaxes (which in our trellis look like light gray arrows) are often called \"backpointers\". The process of using the backpointers to recover the token sequence of the most likely alignment is known as \"backtracking\".</p> <p>Some special cases:</p> <ul> <li> <p>For the first \\((t=1)\\) column of the Viterbi matrix, we set \\(v(s=1,t=1)\\) to \\(p(s=1,t=1)\\) and all other \\(v(s&gt;1,t=1)\\) to 0. We do this because all possible alignments must start with the first token in the ground truth \\((s=1)\\). (This only holds for our current setup, and would be different in a CTC setup, see below).</p> </li> <li> <p>For some values of \\((s,t)\\), either the \\((s-1, t-1)\\) or \\((s, t-1)\\) nodes are not reachable, in which cases we ignore their terms in the \\(v(s,t)\\) formula, and do a trivial max over one element.</p> </li> </ul>"},{"location":"blogs/2023/2023-08-forced-alignment/#extending-to-ctc","title":"Extending to CTC","text":"<p>The example above was simplified from a CTC approach to make it easier to understand and visualize. If you want to work with CTC alignments, you must allow blank tokens in between your ground truth tokens (the blank tokens are always optional except for in between repeated tokens.)</p> <p>Thus, if the reference text tokens are still <code>'C', 'A', 'T'</code>, we add optional 'blank' tokens in between them, which we denote as <code>&lt;b&gt;</code>. The diagram of the allowed sequence of tokens would look like this:</p> <p> </p> Every possible alignment passes from \"START\" to \"END\" with T stops at each red token <p>As you can see, <code>S</code> \u2014 the total number of tokens \u2014 is 7 (3 non-blank tokens and 4 blank tokens). If we keep \\(T=5\\), the trellis for CTC with reference text tokens <code>'C', 'A', 'T'</code> would look like this:</p> <p> </p> The shape of the trellis if we use a CTC model, our reference text tokens are 'C', 'A', 'T', and the number of timesteps in the ASR model output (T) is 5. <p>The Viterbi algorithm rules would also change, as follows:</p> <p> </p> The formula we use to calculate v(s,t) probabilities for a CTC model. <p>However, the principle remains the same: we initialize a Viterbi matrix of size \\(S \\times T\\) and fill it in according to the recursive formula. Once we fill it in, because there are 2 valid final tokens, we need to compare \\(v(s=S-1,t=T)\\) and \\(v(s=S,t=T)\\) - the token with the higher value is the end token of the most likely probability. To recover the overall most likely alignment, we need to backtrack from the higher of \\(v(s=S-1,t=T)\\) or \\(v(s=S,t=T)\\).</p>"},{"location":"blogs/2023/2023-08-forced-alignment/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we have shown how to do forced alignment using the Viterbi algorithm, which is an efficient way to find the most likely path through the reference text tokens.</p> <p>You can obtain forced alignments using the NeMo Forced Aligner (NFA) tool within NeMo, which has an efficient PyTorch tensor-based implementation of Viterbi decoding on CTC models. An efficient CUDA-based implementation of Viterbi decoding was also recently added to TorchAudio, though it is currently only available in the nightly version of TorchAudio, and is not always faster than the current NFA PyTorch tensor-based implementation.</p> <p>Although our examples used characters are tokens, most NeMo models use sub-word tokens, such as in the diagram below. Furthermore, although we've given examples using probabilities ranging from 0 to 1, most Viterbi algorithms operate on log probabilities, which will make the operations in the algorithm more numerically stable. </p> <p> </p> The NFA forced alignment pipeline, which has been described in this blog post. <p>To learn more about NFA, you can now refer to the resources here.</p> <ol> <li> <p>Specifically we will be explaining how to use CTC-like (Connectionist Temporal Classification) models which output a probability distribution over vocabulary tokens per audio timestep. We will explain how forced alignment works using a simplified CTC-like model, and explain how to extend it to a CTC model at the end of the tutorial. There are many CTC models available out of the box in NeMo. Alternative types of ASR models inlcude Transducer models (also available in NeMo), and attention-based encoder-decoder models (many of which build on this work, and a recent example of which is Whisper). These types of models would require a different approach to obtaining forced alignments.\u00a0\u21a9</p> </li> <li> <p>This video is of an excerpt from 'The Jingle Book' by Carolyn Wells. The audio is a reading of a poem called \"The Butter Betty Bought\". The audio is taken from a LibriVox recording of the book. We used NeMo Forced Aligner to generate the subtitle files for the video. The text was adapted from Project Gutenberg. Both the original audio and the text are in the public domain.\u00a0\u21a9</p> </li> <li> <p>There are toolkits specialized for this purpose such as CTC Segmentation, which has an integration in NeMo. It uses an extended version of the algorithm that we describe in this tutorial. The key difference is that the algorithm in this tutorial is forced alignment which assumes that the ground truth text provided is exactly what is spoken in the text. However, in practice the ground truth text may be different from what is spoken, and the algorithm in CTC Segmentation accounts for this.\u00a0\u21a9</p> </li> <li> <p>These can be graphemes (i.e. letters or characters), phonemes, or subword-tokens\u00a0\u21a9</p> </li> <li> <p>Such as CTC Segmentation (also integrated in NeMo), Gentle aligner.\u00a0\u21a9</p> </li> <li> <p>If you want to learn more about how ASR models are trained, this is an excellent tutorial.\u00a0\u21a9</p> </li> <li> <p>In the graph, we can also try to follow a 'greedy' path where we only take the outgoing path with the highest probability. In this example, this 'greedy' approach would give us the alignment path <code>CCATT</code>, which has a probability of 0.020 - lower than the actual most likely path.\u00a0\u21a9</p> </li> <li> <p>To estimate the number of tokens, we assume there are 2 tokens per word \\(\\implies 10\\) words \\(\\times 2\\) tokens/word = 20 tokens. </p> <p>To estimate the number of timesteps, we assume a spectrogram frame hop size of 0.01 \\(\\implies \\frac{5}{0.01} = 500 = T_{in} \\implies {T} = \\frac{T_{in}}{8} = \\frac{500}{8} = 62.5 \\approx 63\\).\u00a0\u21a9</p> </li> <li> <p>The exact number of possible paths in our formulation is equal to \\({T-1 \\choose S-1 }\\), and if \\(T=63\\) and \\(S=20\\), then \\({T-1 \\choose S-1 }={63-1 \\choose 20-1 }={62 \\choose 19 }=4.8 \\times 10^{15}\\).</p> <p>We will explain how we we deduced the formula \\({T-1 \\choose S-1 }\\) below.</p> <p>The number of possible paths is the same as the number of ways we could fit \\(S\\) tokens into \\(T\\) boxes, with the \\(S\\) tokens being in some fixed order.</p> <p>This formulation is the same as having \\(T\\) boxes, and putting \\(S-1\\) markers in between the boxes, where the markers indicate a switch from one token to the next. There are \\(T-1\\) locations where the markers can go (i.e. \\(T-1\\) spaces between the boxes), therefore the number of possible ways we could arrange the markers is \\({T-1 \\choose S-1 }\\). Therefore, this is also the number of possible alignment paths in our setup with \\(S\\) tokens and \\(T\\) timesteps.</p> <p>This is analogous to a known result in combinatorics called stars and bars - in our case our 'boxes' are the 'stars' and our 'markers' are the 'bars'.\u00a0\u21a9</p> </li> <li> <p>Each entry within the same column is actually independent of the other entries in that column. We can compute those entries in any order or, even better, simultaneously - which would speed up the time to complete the algorithm.\u00a0\u21a9</p> </li> </ol>"},{"location":"blogs/2023/2023-08-nfa/","title":"Introducing NeMo Forced Aligner","text":"<p>Today we introduce NeMo Forced Aligner: a NeMo-based tool for forced alignment.</p> <p>NFA allows you to obtain token-level, word-level and segment-level timestamps for words spoken in an audio file. NFA produces timestamp information in a variety of output file formats, including subtitle files, which you can use to create videos such as the one below1:</p> <p> </p> Video with words highlighted according to word alignment timestamps obtained with NFA <p>Ways to get started:</p> <ul> <li>Try out our HuggingFace Space demo to quickly test NFA in various languages.</li> <li>Follow along with our step-by-step NFA \"how-to\" tutorial.</li> <li>Learn more about how forced alignment works in this explainer tutorial.</li> </ul> <p>You can also download NFA from the NeMo repository.</p> <p>You can use NFA timestamps to:</p> <ul> <li>Split audio files into shorter segments</li> <li>Generate token- or word-level subtitles, like in our HuggingFace Space</li> <li>Train token/word duration components of text-to-speech or speaker diarization models</li> </ul> <p>NFA alignment timestamps can be based on reference text that you provide, or reference text obtained from speech-to-text transcription from a NeMo model. NFA works on audio in 14+ languages: it will work any of the 14 (and counting) languages for which there is an open-sourced NeMo speech-to-text model checkpoint, or you can train your own ASR model for a new language.</p> <p> </p> The NFA forced alignment pipeline <ol> <li> <p>This video is of an excerpt from 'The Jingle Book' by Carolyn Wells. The audio is a reading of a poem called \"The Butter Betty Bought\". The audio is taken from a LibriVox recording of the book. We used NeMo Forced Aligner to generate the subtitle files for the video. The text was adapted from Project Gutenberg. Both the original audio and the text are in the public domain.\u00a0\u21a9</p> </li> </ol>"},{"location":"techblog/techblog-2022/","title":"NeMo Conversational AI Blog Posts and Announcements","text":""},{"location":"techblog/techblog-2022/#interspeech-2022-september","title":"Interspeech 2022 (September)","text":"<ul> <li>Improving Japanese Language ASR by Combining Convolutions with Attention Mechanisms</li> <li>Changing CTC Rules to Reduce Memory Consumption in Training and Decoding</li> <li>Dynamic Scale Weighting Through Multiscale Speaker Diarization</li> <li>Text Normalization and Inverse Text Normalization with NVIDIA NeMo</li> </ul>"},{"location":"techblog/techblog-2022/#august-2022","title":"August 2022","text":"<ul> <li>Solving Automatic Speech Recognition Deployment Challenges</li> </ul>"},{"location":"techblog/techblog-2022/#september-2022","title":"September 2022","text":"<ul> <li>Simplifying Model Development and Building Models at Scale with PyTorch Lightning and NGC</li> </ul>"},{"location":"techblog/techblog-2022/#october-2022","title":"October 2022","text":"<ul> <li>Building an Automatic Speech Recognition Model for the Kinyarwanda Language</li> <li>Making an NVIDIA Riva ASR Service for a New Language</li> </ul>"},{"location":"techblog/techblog-2022/#november-2022","title":"November 2022","text":"<ul> <li>Deploying a 1.3B GPT-3 Model with NVIDIA NeMo Megatron</li> <li>Speech Recognition: Generating Accurate Domain-Specific Audio Transcriptions Using NVIDIA Riva</li> </ul>"},{"location":"techblog/techblog-2022/#december-2022","title":"December 2022","text":"<ul> <li>Training and Tuning Text-to-Speech with NVIDIA NeMo and W&amp;B</li> <li>Deep Learning is Transforming ASR and TTS Algorithms</li> </ul>"},{"location":"techblog/techblog-2023/","title":"NeMo Conversational AI Blog Posts and Announcements","text":""},{"location":"techblog/techblog-2023/#slt-2022-january","title":"SLT 2022 (January)","text":"<ul> <li>Entropy-Based Methods for Word-Level ASR Confidence Estimation</li> <li>Controlled Adaptation of Speech Recognition Models to New Domains</li> </ul>"},{"location":"techblog/techblog-2023/#january-2023","title":"January 2023","text":"<ul> <li>Autoscaling NVIDIA Riva Deployment with Kubernetes for Speech AI in Production</li> <li>Multilingual and Code-Switched Automatic Speech Recognition with NVIDIA NeMo</li> </ul>"}]}